# ProcessPulse Configuration
# Copy this file to .env and customize as needed
# cp env.example .env

# ===========================================
# PORT CONFIGURATION
# ===========================================
# Change these if ports conflict with existing services

FRONTEND_PORT=80          # Web interface (change to 8080 if 80 is taken)
OLLAMA_PORT=11434         # Ollama API (for external access)
PERPLEXICA_PORT=3000      # Perplexica web search

# ===========================================
# AI MODEL CONFIGURATION
# ===========================================
# Models are downloaded automatically on first run
# 
# CHAT MODEL OPTIONS (pick based on your hardware):
#   - llama3.1:8b      (~4.7 GB) - Recommended, good balance
#   - mistral:7b       (~4.1 GB) - Fast, good quality
#   - llama3.1:70b     (~40 GB)  - Best quality, needs 48GB+ RAM
#   - phi3:medium      (~7.9 GB) - Microsoft's model
#
# EMBEDDING MODEL OPTIONS:
#   - nomic-embed-text (~275 MB) - Recommended, fast
#   - bge-m3           (~1.2 GB) - Higher quality

CHAT_MODEL=llama3.1:8b
EMBEDDING_MODEL=nomic-embed-text

# ===========================================
# DEVELOPMENT/DEBUG
# ===========================================
DEBUG=false

# ===========================================
# OPTIONAL: EXTERNAL OLLAMA
# ===========================================
# If you already have Ollama running on your machine,
# you can point to it instead of using the Docker version.
# Uncomment and set:
#
# OLLAMA_EXTERNAL=true
# OLLAMA_BASE_URL=http://host.docker.internal:11434

# ===========================================
# OPTIONAL: GPU SUPPORT
# ===========================================
# For NVIDIA GPU acceleration, edit docker-compose.yml
# and uncomment the GPU section under the ollama service.
# Requires: nvidia-docker2 package installed

